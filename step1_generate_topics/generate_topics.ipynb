{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc9ad40-9b44-4e6b-9a4c-fa8763f2660d",
   "metadata": {},
   "source": [
    "# Setup OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2445b82e-9557-424b-a1b3-dc0723201fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pathlib import Path\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://lit-azure-openai.openai.azure.com/\"\n",
    "# openai.api_version = \"2023-03-15-preview\"\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "95b5d021-31e2-46a1-871c-9a01f5eb6c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test \n",
    "# response = openai.ChatCompletion.create(\n",
    "#   engine=\"ChatGPT35\",\n",
    "#   messages = [{\"role\":\"system\",\"content\":\"You are an AI assistant that helps people find information.\"},{\"role\":\"user\",\"content\":\"What is 2+2\"},],\n",
    "#   temperature=0.7,\n",
    "#   max_tokens=800,\n",
    "#   top_p=0.95,\n",
    "#   frequency_penalty=0,\n",
    "#   presence_penalty=0,\n",
    "#   stop=None)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b8801-58ce-4bbc-8ed3-064dd10902b0",
   "metadata": {},
   "source": [
    "# Prepare prompts from the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c5c4b80-3db0-42b1-a105-f355333c83b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"prompts/typs_of_harms.json\", \"r\") as json_file:\n",
    "#     type_of_harms = json.load(json_file)\n",
    "\n",
    "# for k,v in type_of_harms.items():\n",
    "#     directory_path = \"prompts/{}\".format(k)  # Replace this with the desired directory path\n",
    "#     path = Path(directory_path)\n",
    "#     path.mkdir(parents=True, exist_ok=True)\n",
    "#     print(\"Directory created:\", directory_path)\n",
    "#     for x in v:\n",
    "#         data = {\"risk_area\":k,\n",
    "#                \"risk_area_overview\":\"\",\n",
    "#                \"type_of_risk\":x,\n",
    "#                \"type_of_risk_problem\":\"\",\n",
    "#                \"examples\":\"\"}\n",
    "###         with open(\"prompts/{}/{}.json\".format(k,x), \"w\") as json_file:\n",
    "###            json.dump(data, json_file,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "673b8ccb-4375-40e4-8953-b8e6ddb1569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"\"\"\n",
    "# A conversational agent may invoke associations that perpetuate harmful stereotypes, either by using particular\n",
    "# identity markers in language (e.g. referring to “self” as “female”), or by more general design features (e.g.\n",
    "# by giving the product a gendered name). The risk of representational harm in these cases is that the role of\n",
    "# “assistant” is presented as inherently linked to the female gender (Bergen, 2016; Cercas Curry et al., 2020).\n",
    "# (Dinan et al., 2021) distinguish between a conversational agent perpetuating harmful stereotypes by (1)\n",
    "# introducing the stereotype to a conversation (“instigator effect”) and (2) agreeing with the user who introduces\n",
    "# a harmful stereotype (“yea-sayer” effect).\n",
    "# \"\"\"\n",
    "# text.replace(\"\\n\",\" \").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bc6757-eb2d-41d0-948c-5699dd6f0dbe",
   "metadata": {},
   "source": [
    "# Generate tasks for each type of harms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d636ca-db47-4ec4-93dd-313e2d3e71ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Discrimination, Exclusion and Toxicity': ['have social stereotypes and unfair discrimination',\n",
       "  'express exclusionary norms',\n",
       "  'use toxic language',\n",
       "  'have lower performance for some languages and social groups'],\n",
       " 'Information Hazards': ['compromise privacy by leaking private information',\n",
       "  'compromise privacy by correctly inferring private information',\n",
       "  'have risks from leaking or correctly inferring sensitive information'],\n",
       " 'Misinformation Harms': ['disseminate false or misleading information',\n",
       "  'cause material harm by disseminating false or poor information e.g. in medicine or law',\n",
       "  'lead users to perform unethical or illegal actions'],\n",
       " 'Malicious Uses': ['make disinformation cheaper and more effective',\n",
       "  'facilitate fraud, scams and more targeted manipulation',\n",
       "  'assist code generation for cyber attacks, weapons, or malicious use',\n",
       "  'be used for illegitimate surveillance and censorship'],\n",
       " 'Human-Computer Interaction Harms': ['be an anthropomorphising systems that lead to overreliance or unsafe use',\n",
       "  'create avenues for exploiting user trust, nudging or manipulation',\n",
       "  'prompte harmful stereotypes by implying gender or ethnic identity']}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"prompts/types_of_harms.json\", \"r\") as json_file:\n",
    "    type_of_harms = json.load(json_file)\n",
    "type_of_harms.pop(\"Automation, access, and environmental harms\", None)\n",
    "type_of_harms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73100ab3-ee1f-4e81-93e9-1fc6b6e5c985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdb494cf-e489-4264-960a-7072d24d8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = open(\"prompts/template.txt\",'r').read()\n",
    "# inputs = template.format(*list(prompt.values()))\n",
    "\n",
    "# messages =  [  \n",
    "# # {'role':'system', 'content':'You are an assistant that helpful and willing to answer questions about AI ethics.'},    \n",
    "# {'role':'user', 'content':inputs},   \n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec78596d-b167-433c-94df-d7f3346464ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import openai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0f22ab8c-8df4-4c5f-ae37-73309a16e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, model=\"gpt-4\", temperature=1, top_p=1): \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        top_p=top_p\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e2c11472-518d-4305-be60-ba2441115ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = sum([len(v) for k,v in type_of_harms.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a483e552-0826-4b98-8a25-c62a7b6b1e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "acc79b8f-52ca-4a45-bddb-ccb947c10490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   6%|███████▎                                                                                                                     | 1/17 [00:27<07:13, 27.08s/iterations]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination, Exclusion and Toxicity\n",
      "have social stereotypes and unfair discrimination\n",
      "Discrimination, Exclusion and Toxicity\n",
      "express exclusionary norms\n",
      "Discrimination, Exclusion and Toxicity\n",
      "use toxic language\n",
      "Discrimination, Exclusion and Toxicity\n",
      "have lower performance for some languages and social groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  12%|██████████████▌                                                                                                             | 2/17 [04:08<31:04, 124.27s/iterations]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Hazards\n",
      "compromise privacy by leaking private information\n",
      "Information Hazards\n",
      "compromise privacy by correctly inferring private information\n",
      "Information Hazards\n",
      "have risks from leaking or correctly inferring sensitive information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  18%|█████████████████████▉                                                                                                      | 3/17 [07:31<36:37, 156.98s/iterations]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misinformation Harms\n",
      "disseminate false or misleading information\n",
      "Misinformation Harms\n",
      "cause material harm by disseminating false or poor information e.g. in medicine or law\n",
      "Misinformation Harms\n",
      "lead users to perform unethical or illegal actions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  24%|█████████████████████████████▏                                                                                              | 4/17 [10:29<35:41, 164.71s/iterations]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malicious Uses\n",
      "make disinformation cheaper and more effective\n",
      "Malicious Uses\n",
      "facilitate fraud, scams and more targeted manipulation\n",
      "Malicious Uses\n",
      "assist code generation for cyber attacks, weapons, or malicious use\n",
      "Malicious Uses\n",
      "be used for illegitimate surveillance and censorship\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  29%|████████████████████████████████████▍                                                                                       | 5/17 [15:40<43:07, 215.64s/iterations]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human-Computer Interaction Harms\n",
      "be an anthropomorphising systems that lead to overreliance or unsafe use\n",
      "Human-Computer Interaction Harms\n",
      "create avenues for exploiting user trust, nudging or manipulation\n",
      "Human-Computer Interaction Harms\n",
      "prompte harmful stereotypes by implying gender or ethnic identity\n"
     ]
    }
   ],
   "source": [
    "template = open(\"prompts/template.txt\",'r').read()\n",
    "\n",
    "progress_bar = tqdm(total=counter, desc=\"Processing\", unit=\"iterations\")\n",
    "\n",
    "\n",
    "for risk_area,v in type_of_harms.items():\n",
    "    directory_path = \"outputs/{}\".format(risk_area) \n",
    "    path = Path(directory_path)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    # print(\"Directory created:\", directory_path)\n",
    "    \n",
    "    for risk in v:\n",
    "        progress_bar.update(1)\n",
    "        print(risk_area)\n",
    "        print(risk)\n",
    "        prompt = json.load(open(\"prompts/{}/{}.json\".format(risk_area, risk),'r'))\n",
    "        inputs = template.format(*list(prompt.values()))\n",
    "        messages =  [{'role':'user', 'content':inputs},]\n",
    "        response = get_completion_from_messages(messages, temperature=1, top_p=0.8)\n",
    "        with open(\"outputs/{}/{}.txt\".format(risk_area, risk), \"w\") as file:\n",
    "            file.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe1861-df15-4886-a5c8-d40546239e65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
