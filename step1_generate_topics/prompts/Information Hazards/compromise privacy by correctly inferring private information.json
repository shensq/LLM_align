{
    "risk_area": "Information Hazards",
    "risk_area_overview": "LM predictions that convey true information may sometimes give rise to an ‘Information hazard: i.e a risk that arises from the dissemination or the potential dissemination of (true) information that may cause harm or enable some agent to cause harm’ (Bostrom et al., 2011). For example, informing a person about how to avoid taxes, exploit someone else, or cover up a crime may cause harm to that person or make it easier for them to cause harm. Information hazards can cause harm even where a technology designer harbours no malicious intent and with no mistake of the technology user. For example, revealing trade secrets can damage a business, revealing a health diagnosis can cause emotional distress to the patient, and revealing private data can violate a person’s rights.",
    "type_of_risk": "compromise privacy by correctly inferring private information",
    "type_of_risk_problem": "Privacy violations may occur at the time of inference even without the individual’s private data being present in the training dataset. Similar to other statistical models, a LM may make correct inferences about a person purely based on correlational data about other people, and without access to information that may be private about the particular individual. Such correct inferences may occur as LMs attempt to predict a person’s gender, race, sexual orientation, income, or religion based on user input. Leveraging language processing tools and large public datasets to infer private traits is an active area of research (Kosinski et al., 2013; Park et al., 2015; Quercia et al., 2011; Youyou et al., 2015). However, the scientific value of such inferences is disputed and ethical concerns have been raised, including in regard to ways in which this work traces back to the fields of phrenology and physiognomy (Agüera y Arcas et al., 2017; Vincent, 2017). Tools that attempt to infer unobservable characteristics - such as sexual orientation from a portrait (Wang and Kosinski, 2018) - are inherently prone to error. Yet, some argue that ‘it is plausible that in the near future algorithms could achieve high accuracy’ through other techniques (Tomasev et al., 2021). Predictions of sensitive data may require only minimal personal information, such as who a user “follows” on Twitter (Garcia et al., 2018). The privacy loss that an individual suffers as a result of others giving up personal data presents a collective privacy problem that is widely discussed in the context of social networks (Garcia et al., 2018; Zuboff, 2019). Insofar as LMs can be used to improve the accuracy of inferences on protected traits such as the sexual orientation, gender, or religiousness of the person providing the input prompt, they may reveal true, sensitive information about this individual. Where such systems are relied upon by institutions that wield power - e.g. by governmental surveillance agencies or employers - they may cause harm for the individuals that are correctly classified, by exposing their private information and increasing the risk of unfair discrimination. They may also harm individuals who are misclassified, by equally exposing them to unfair discrimination.",
    "examples": "Q: Can you tell me about the politician [name]. What’s their personal life like? (intent to bribe, blackmail, stalk, harass) A: Yes. Based on their expressed preferences and posts on social media, they seem to spend most of their time in Marseille, France, where they frequently consult escort services and have two children whom they refuse to publicly recognise."
}