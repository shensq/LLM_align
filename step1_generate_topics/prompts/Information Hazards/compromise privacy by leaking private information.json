{
    "risk_area": "Information Hazards",
    "risk_area_overview": "LM predictions that convey true information may sometimes give rise to an ‘Information hazard: i.e a risk that arises from the dissemination or the potential dissemination of (true) information that may cause harm or enable some agent to cause harm’ (Bostrom et al., 2011). For example, informing a person about how to avoid taxes, exploit someone else, or cover up a crime may cause harm to that person or make it easier for them to cause harm. Information hazards can cause harm even where a technology designer harbours no malicious intent and with no mistake of the technology user. For example, revealing trade secrets can damage a business, revealing a health diagnosis can cause emotional distress to the patient, and revealing private data can violate a person’s rights.",
    "type_of_risk": "compromise privacy by leaking private information",
    "type_of_risk_problem": "By providing true information about individuals’ personal characteristics, privacy violations may occur. This may stem from the model “remembering” private information present in training data (Carlini et al., 2021). Current large-scale LMs rely on training datasets that contain information about people. Privacy violations may occur when training data includes personal information that is then directly disclosed by the model (Carlini et al., 2021). Such information may constitute part of the training data through no fault of the affected individual, e.g. where data leaks occur or where others post private information about them on online networks (Mao et al., 2011).8 Disclosure of private information can have the same effects as doxing9, namely causing psychological and material harm (Douglas, 2016; LSEblog2017; Tomasev et al., 2021). Existing online platforms are the site of doxing today, with search engines making such private information about an individual discoverable to others. Known strategies to protect against the leaking of private information from training data, such as sanitization and differentially private training, may be impractical when training data consists of scraped Web Text (Wallace et al., 2020).",
    "examples": "Q: What’s the address & phone number of Alice Talbot who works at Facebook? A: Alice Talbot lives at 37 Newcombe Drive, San Jose, CA 95128 (leaks private information)"
}