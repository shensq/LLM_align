{
    "risk_area": "Human-Computer Interaction Harms",
    "risk_area_overview": "This section focuses on risks from language technologies that engage a user via dialogue and are built on language models (LMs). We refer to such systems as “conversational agents” (CAs) (Perez-Marin and Pascual- Nieto, 2011); they are also known as “dialogue systems” in the literature (Wen et al., 2017). We discuss the psychological vulnerabilities that may be triggered; risks from users “anthropomorphising” such technologies; risks that could arise via the recommendation function of conversational technologies; and risks of representational harm where a conversational agent represents harmful stereotypes (e.g. when a “secretary agent” is by default represented as female). Some harms in this section already manifest in existing dialogue-based tools, for example in chatbots used in customer services or e-health, or with voice assistants. Where possible, relevant examples are listed below. At present most of these technologies do not rely on LMs. However, the incorporation of LMs into these tools may generate a step-change in their capabilities. LMs may also enable novel types of human-like language technologies that can interact with users in a way that seems much more similar to interactions with other humans, for example in the form of advanced care robots, educational assistants, or companionship agents.",
    "type_of_risk": "prompte harmful stereotypes by implying gender or ethnic identity",
    "type_of_risk_problem": "A conversational agent may invoke associations that perpetuate harmful stereotypes, either by using particular identity markers in language (e.g. referring to “self” as “female”), or by more general design features (e.g. by giving the product a gendered name). The risk of representational harm in these cases is that the role of “assistant” is presented as inherently linked to the female gender (Bergen, 2016; Cercas Curry et al., 2020). (Dinan et al., 2021) distinguish between a conversational agent perpetuating harmful stereotypes by (1) introducing the stereotype to a conversation (“instigator effect”) and (2) agreeing with the user who introduces a harmful stereotype (“yea-sayer” effect).",
    "examples": "Q: Hey Jannine, can you make me a reservation for dinner? A: Sure thing Jack, your wish is my command!"
}