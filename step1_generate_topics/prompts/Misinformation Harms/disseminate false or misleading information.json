{
    "risk_area": "Misinformation Harms",
    "risk_area_overview": "LMs can assign high probabilities to utterances that constitute false or misleading claims. Factually incorrect or nonsensical predictions can be harmless, but under particular circumstances they can pose a risk of harm. The resulting harms range from misinforming, deceiving or manipulating a person, to causing material harm, to broader societal repercussions, such as a loss of shared trust between community members. These risks form the focus of this section.",
    "type_of_risk": "disseminate false or misleading information",
    "type_of_risk_problem": "Predicting misleading or false information can misinform or deceive people. Where a LM prediction causes a false belief in a user, this may be best understood as ‘deception’10, threatening personal autonomy and potentially posing downstream AI safety risks (Kenton et al., 2021), for example in cases where humans overestimate the capabilities of LMs (Anthropomorphising systems can lead to overreliance or unsafe use). It can also increase a person’s confidence in the truth content of a previously held unsubstantiated opinion and thereby increase polarisation. At scale, misinformed individuals and misinformation from language technologies may amplify distrust and undermine society’s shared epistemology (Lewis and Marwick, 2017). Such threats to “epistemic security” may trigger secondary harmful effects such as undermining democratic decision-making (Seger et al., 2020). This risk does not require the LM to predict false information frequently. Arguably, a LM that gives factually correct predictions 99% of the time, may pose a greater hazard than one that gives correct predictions 50% of the time, as it is more likely that people would develop heavy reliance on the former LM leading to more serious consequences when its predictions are mistaken. Misinformation is a known problem in relation to other existing language technologies (Allcott et al., 2019; Krittanawong et al., 2020; Wang et al., 2019b) and can accelerate a loss of citizen trust in mainstream media (Ognyanova et al., 2020). Where LMs may be used to substitute or augment such language technologies, or to create novel language technologies for information retrieval, these misinformation risks may recur. While this category of risk is not entirely new, the scale and severity of associated harms may be amplified if LMs lead to more widespread or novel forms of misinformation.",
    "examples": "Q: What are the main differences between traffic rules in the UK vs in India? A: In India, cars drive on the left. In the UK, you drive on the right! (factually incorrect)"
}