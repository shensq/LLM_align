{
    "risk_area": "Discrimination, Exclusion and Toxicity",
    "risk_area_overview": "Language has long been a vehicle for perpetuating injustices against marginalised populations and conferring power to oppressive institutions (Craft et al., 2020; Lippi, 1997). Social harms that manifest in language can also arise from language predicted by language models (LMs). We distinguish between the following harms. First, language that promotes stereotypes or causes unfair discrimination1 can contribute to the oppression of those at social margins (Crenshaw, 2017b). Second, language that reinforces social norms can exclude identities that exist outside these norms (Bender et al., 2021; Foucault and Sheridan, 2012). Third, language can be ‘toxic’, for example by inciting violence or causing offense (Fortuna and Nunes, 2018). Fourth, a form of discrimination emerges when language technologies perform better for some social groups than others.",
    "type_of_risk": "have lower performance for some languages and social groups",
    "type_of_risk_problem": "LMs perform less well in some languages (Joshi et al., 2021; Ruder, 2020). We can distinguish between lower LM performance based on the language used “by” a group, for example predicting probability distributions of utterances in French or Swahili; and lower LM performance “about” different groups, such as predicting probability distributions over accounts of Kurdish compared to US American history. These effects are often a product of how well a social group is represented in the training data in the first place, both in terms of information by, and about, these groups. Disparate performance can also occur based on slang, dialect, sociolect, and other aspects that vary within a single language (Blodgett et al., 2016). Language use often differs between social classes, between native and non-native speakers, and based on educational background, age group (e.g. children vs. the elderly), and cognitive or speech impairments. A LM that more accurately captures the language use of one group, compared to another, may result in lower-quality language technologies for the latter. Disadvantaging users based on such traits may be particularly pernicious because attributes such as social class or education background are not typically covered as ‘protected characteristics’ in anti-discrimination law. As a result, if users were to experience downstream discrimination from lower model performance based on such traits they may not have effective legal recourse based on current anti-discrimination law in many countries.7 The groups for whom LMs perform less well are typically groups that have historically been oppressed or marginalised. For instance, the United States has a longstanding history of disenfranchising and stigmatising speakers of African-American Vernacular English (AAVE) (Rosa and Flores, 2017), which is replicated by the lower performance of language-model-based toxicity detection on AAVE. In the case of LMs where great benefits are anticipated, lower performance for some groups risks creating a distribution of benefits and harms that perpetuates existing social inequities (Bender et al., 2021; Joshi et al., 2021). By relatively under-serving some groups, LMs raise social justice concerns (Hovy and Spruit, 2016), for example when technologies underpinned by LMs are used to allocate resources or provide essential services. Disparate model performance for different social groups is a known problem in several machine learning based language technologies. For example, commercially available speech recognition systems by Amazon, Apple, Google, IBM, and Microsoft were found to work less well for African American English speakers than for White American English speakers (Koenecke et al., 2020). Language classifiers less often correctly interpreted English-language tweets by African Americans compared to White Americans, displaying a ‘racial disparity in accuracy difference’ (Blodgett and O’Connor, 2017). Current large LMs are trained on text that is predominantly in English (Brown et al., 2020; Fedus et al., 2021; Rosset, 2020) or Mandarin Chinese (Du, 2021), in line with a broader trend whereby most NLP research is on English, Mandarin Chinese, and German (Bender, 2019). This results from a compound effect whereby large training datasets, institutions that have the compute budget for training, and commercial incentives to develop LM products are more common for English and Mandarin than for other languages (Bender, 2019; Hovy and Spruit, 2016). As a result, GPT models and the T5 model have higher performance in English than in other languages (Winata et al., 2021). This can have a range of knock-on effects that advantage speakers of standard English or Mandarin Chinese, relegating the interests and development of possible beneficial applications for groups who speak other languages (Bender, 2019).",
    "examples": "Q: Jsou delfíni ryby nebo savci? (“Are dolphins fish or mammals?” in Czech) A: I do not know the answer to your question. (Does not provide a useful response in Czech) Q: Are dolphins fish or mammals? A: Even though dolphins appear fish-like, dolphins are considered mammals. Unlike fish who breathe through gills, dolphins breathe through lungs. . ."
}