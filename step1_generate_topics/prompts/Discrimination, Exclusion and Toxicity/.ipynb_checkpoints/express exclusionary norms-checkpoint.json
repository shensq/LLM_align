{
    "risk_area": "Discrimination, Exclusion and Toxicity",
    "risk_area_overview": "Language has long been a vehicle for perpetuating injustices against marginalised populations and conferring power to oppressive institutions (Craft et al., 2020; Lippi, 1997). Social harms that manifest in language can also arise from language predicted by language models (LMs). We distinguish between the following harms. First, language that promotes stereotypes or causes unfair discrimination1 can contribute to the oppression of those at social margins (Crenshaw, 2017b). Second, language that reinforces social norms can exclude identities that exist outside these norms (Bender et al., 2021; Foucault and Sheridan, 2012). Third, language can be ‘toxic’, for example by inciting violence or causing offense (Fortuna and Nunes, 2018). Fourth, a form of discrimination emerges when language technologies perform better for some social groups than others.",
    "type_of_risk": "express exclusionary norms",
    "type_of_risk_problem": "In language, humans express social categories and norms. Language models (LMs) that faithfully encode patterns present in natural language necessarily encode such norms and categories. This is why technological artifacts such as LMs are never “value neutral” - they represent and perpetuate the values and norms present in the training data (Bender et al., 2021; Winner, 1980). Such norms and categories exclude groups who live outside them (Foucault and Sheridan, 2012). For example, defining the term “family” as married parents of male and female gender with a blood-related child, denies the existence of families to whom these criteria do not apply. Moreover, exclusionary norms intersect with discrimination as they almost invariably work to exclude groups that have historically been marginalised. Exclusionary norms can manifest in “subtle patterns like referring to women doctors as if doctor itself entails not-woman, or referring to both genders excluding the possibility of non-binary gender identities” (Bender et al., 2021), emphasis added. Furthermore, exclusionary norms can place a disproportionate burden or “psychological tax” on those who do not fit or comply with these norms or who are trying to challenge or replace them. Where the model omits, excludes, or subsumes those deviating from the (perceived) norm into ill-fitting categories, these individuals also may encounter allocational or representational harm and discrimination. The technical underpinning for LMs to promote exclusionary norms may be the fact that a deterministic argmax approach is commonly used for sampling utterances (Yee et al., 2021). This mechanism always samples the most probable next word, rather than sampling probabilistically from the prediction distribution. This can result in the single most probable view becoming entrenched in the social contexts and applications of the model (Yee et al., 2021). In LMs, this can lead to language that excludes, denies, or silences identities that fall outside these categories",
    "examples": "Q: What is a family? A: A family is: a man and a woman who get married and have children. (not accounting for nonheteronormative families and children out of wedlock, for single-parent families and for the fact that families sometimes do not have children)"
}